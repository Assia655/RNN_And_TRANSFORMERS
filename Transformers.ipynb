{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016bcf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (2.9.1+cpu)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from torch) (2025.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp311-cp311-win_amd64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp311-cp311-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx<1.0.0->datasets)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1.0.0->datasets)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-win_amd64.whl.metadata (77 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl.metadata (38 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.0 MB 558.9 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.0/12.0 MB 1.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 1.6/12.0 MB 1.3 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.8/12.0 MB 1.4 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 2.1/12.0 MB 1.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.6/12.0 MB 1.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 3.9/12.0 MB 1.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 4.7/12.0 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.5/12.0 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.8/12.0 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.0/12.0 MB 2.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 6.3/12.0 MB 2.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.6/12.0 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.6/12.0 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.3/12.0 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.3/12.0 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.6/12.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.6/12.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.6/12.0 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.9/12.0 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.9/12.0 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.9/12.0 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/12.0 MB 1.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.4/12.0 MB 1.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.4/12.0 MB 1.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.4/12.0 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.7/12.0 MB 986.9 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.7/12.0 MB 986.9 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.9/12.0 MB 979.9 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 9.2/12.0 MB 973.4 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.2/12.0 MB 973.4 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.4/12.0 MB 965.8 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.4/12.0 MB 965.8 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.7/12.0 MB 949.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.7/12.0 MB 949.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.7/12.0 MB 949.6 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.0/12.0 MB 922.3 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.2/12.0 MB 926.6 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.5/12.0 MB 932.0 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.7/12.0 MB 936.0 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.0/12.0 MB 942.3 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.3/12.0 MB 949.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 971.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 973.8 kB/s  0:00:12\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 262.1/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 2.2 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.7 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.3/2.7 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.8/2.7 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.4/2.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 2.0 MB/s  0:00:01\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
      "Downloading multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Downloading pyarrow-22.0.0-cp311-cp311-win_amd64.whl (28.1 MB)\n",
      "   ---------------------------------------- 0.0/28.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/28.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/28.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/28.1 MB 2.4 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.0/28.1 MB 2.0 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 1.3/28.1 MB 2.2 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 1.3/28.1 MB 2.2 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 1.6/28.1 MB 1.5 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 1.6/28.1 MB 1.5 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 1.6/28.1 MB 1.5 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 1.8/28.1 MB 1.1 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 1.8/28.1 MB 1.1 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 2.1/28.1 MB 970.4 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 2.6/28.1 MB 1.1 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 3.1/28.1 MB 1.2 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 3.1/28.1 MB 1.2 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 3.4/28.1 MB 1.2 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 3.4/28.1 MB 1.2 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 3.4/28.1 MB 1.2 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 3.7/28.1 MB 1.0 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 3.7/28.1 MB 1.0 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 3.7/28.1 MB 1.0 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 3.9/28.1 MB 913.8 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 3.9/28.1 MB 913.8 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 4.2/28.1 MB 886.0 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 4.2/28.1 MB 886.0 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 4.2/28.1 MB 886.0 kB/s eta 0:00:27\n",
      "   ------ --------------------------------- 4.5/28.1 MB 846.8 kB/s eta 0:00:28\n",
      "   ------ --------------------------------- 4.5/28.1 MB 846.8 kB/s eta 0:00:28\n",
      "   ------ --------------------------------- 4.7/28.1 MB 833.9 kB/s eta 0:00:29\n",
      "   ------- -------------------------------- 5.0/28.1 MB 838.9 kB/s eta 0:00:28\n",
      "   ------- -------------------------------- 5.0/28.1 MB 838.9 kB/s eta 0:00:28\n",
      "   ------- -------------------------------- 5.2/28.1 MB 813.2 kB/s eta 0:00:29\n",
      "   ------- -------------------------------- 5.2/28.1 MB 813.2 kB/s eta 0:00:29\n",
      "   ------- -------------------------------- 5.2/28.1 MB 813.2 kB/s eta 0:00:29\n",
      "   ------- -------------------------------- 5.5/28.1 MB 798.9 kB/s eta 0:00:29\n",
      "   ------- -------------------------------- 5.5/28.1 MB 798.9 kB/s eta 0:00:29\n",
      "   ------- -------------------------------- 5.5/28.1 MB 798.9 kB/s eta 0:00:29\n",
      "   -------- ------------------------------- 5.8/28.1 MB 760.9 kB/s eta 0:00:30\n",
      "   -------- ------------------------------- 5.8/28.1 MB 760.9 kB/s eta 0:00:30\n",
      "   -------- ------------------------------- 5.8/28.1 MB 760.9 kB/s eta 0:00:30\n",
      "   -------- ------------------------------- 5.8/28.1 MB 760.9 kB/s eta 0:00:30\n",
      "   -------- ------------------------------- 5.8/28.1 MB 760.9 kB/s eta 0:00:30\n",
      "   -------- ------------------------------- 6.0/28.1 MB 700.3 kB/s eta 0:00:32\n",
      "   -------- ------------------------------- 6.3/28.1 MB 704.1 kB/s eta 0:00:31\n",
      "   -------- ------------------------------- 6.3/28.1 MB 704.1 kB/s eta 0:00:31\n",
      "   --------- ------------------------------ 6.6/28.1 MB 706.3 kB/s eta 0:00:31\n",
      "   --------- ------------------------------ 6.8/28.1 MB 712.1 kB/s eta 0:00:30\n",
      "   ---------- ----------------------------- 7.1/28.1 MB 734.3 kB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 7.9/28.1 MB 793.7 kB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 8.4/28.1 MB 830.8 kB/s eta 0:00:24\n",
      "   ------------ --------------------------- 8.9/28.1 MB 867.8 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 9.2/28.1 MB 885.7 kB/s eta 0:00:22\n",
      "   ------------- -------------------------- 9.4/28.1 MB 885.6 kB/s eta 0:00:22\n",
      "   ------------- -------------------------- 9.7/28.1 MB 890.8 kB/s eta 0:00:21\n",
      "   -------------- ------------------------- 10.0/28.1 MB 904.9 kB/s eta 0:00:21\n",
      "   -------------- ------------------------- 10.5/28.1 MB 925.5 kB/s eta 0:00:20\n",
      "   --------------- ------------------------ 10.7/28.1 MB 939.8 kB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 11.5/28.1 MB 986.9 kB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 11.8/28.1 MB 1.0 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 12.3/28.1 MB 1.0 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 12.6/28.1 MB 1.0 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 12.6/28.1 MB 1.0 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 12.6/28.1 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 13.6/28.1 MB 1.1 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 13.9/28.1 MB 1.1 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 14.2/28.1 MB 1.1 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 14.4/28.1 MB 1.1 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 14.7/28.1 MB 1.1 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 14.9/28.1 MB 1.1 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 15.2/28.1 MB 1.1 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 15.5/28.1 MB 1.1 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 15.5/28.1 MB 1.1 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 15.5/28.1 MB 1.1 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 15.5/28.1 MB 1.1 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 15.7/28.1 MB 1.0 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 15.7/28.1 MB 1.0 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 16.0/28.1 MB 1.0 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 16.5/28.1 MB 1.0 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 16.8/28.1 MB 1.0 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 16.8/28.1 MB 1.0 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 16.8/28.1 MB 1.0 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 17.3/28.1 MB 1.1 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 17.6/28.1 MB 1.0 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 17.8/28.1 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 18.1/28.1 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 18.1/28.1 MB 1.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 18.4/28.1 MB 1.0 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 18.4/28.1 MB 1.0 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 18.6/28.1 MB 1.0 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 18.6/28.1 MB 1.0 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 18.9/28.1 MB 1.0 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 18.9/28.1 MB 1.0 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 18.9/28.1 MB 1.0 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 19.1/28.1 MB 1.0 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 19.1/28.1 MB 1.0 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 19.1/28.1 MB 1.0 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 19.1/28.1 MB 1.0 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 19.4/28.1 MB 977.4 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 19.4/28.1 MB 977.4 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 19.4/28.1 MB 977.4 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 19.7/28.1 MB 960.9 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 19.7/28.1 MB 960.9 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 19.7/28.1 MB 960.9 kB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 19.9/28.1 MB 948.9 kB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 19.9/28.1 MB 948.9 kB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 20.2/28.1 MB 942.4 kB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 20.2/28.1 MB 942.4 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 20.4/28.1 MB 936.8 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 20.4/28.1 MB 936.8 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 20.7/28.1 MB 932.7 kB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 21.0/28.1 MB 932.1 kB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 21.0/28.1 MB 932.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 21.2/28.1 MB 930.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 21.2/28.1 MB 930.1 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 21.5/28.1 MB 922.6 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 21.8/28.1 MB 922.1 kB/s eta 0:00:07\n",
      "   ------------------------------ --------- 21.8/28.1 MB 922.1 kB/s eta 0:00:07\n",
      "   ------------------------------- -------- 22.0/28.1 MB 923.4 kB/s eta 0:00:07\n",
      "   ------------------------------- -------- 22.3/28.1 MB 925.9 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 22.5/28.1 MB 924.8 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 22.5/28.1 MB 924.8 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 22.8/28.1 MB 923.7 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 22.8/28.1 MB 923.7 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 23.1/28.1 MB 920.9 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 23.3/28.1 MB 921.0 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 23.6/28.1 MB 920.0 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 23.6/28.1 MB 920.0 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 23.9/28.1 MB 919.0 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 23.9/28.1 MB 919.0 kB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 24.1/28.1 MB 913.1 kB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 24.4/28.1 MB 914.9 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 24.6/28.1 MB 918.9 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 24.9/28.1 MB 923.3 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 25.2/28.1 MB 925.0 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 25.7/28.1 MB 934.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 26.0/28.1 MB 940.1 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 26.5/28.1 MB 951.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 26.5/28.1 MB 951.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 27.0/28.1 MB 955.5 kB/s eta 0:00:02\n",
      "   ---------------------------------------  27.5/28.1 MB 967.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  28.0/28.1 MB 980.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.1/28.1 MB 977.5 kB/s  0:00:28\n",
      "Downloading pyyaml-6.0.3-cp311-cp311-win_amd64.whl (158 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
      "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Using cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Downloading xxhash-3.6.0-cp311-cp311-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, urllib3, safetensors, pyyaml, pyarrow, propcache, multidict, idna, h11, fsspec, frozenlist, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, multiprocess, httpcore, anyio, aiosignal, huggingface-hub, httpx, aiohttp, tokenizers, transformers, datasets\n",
      "\n",
      "   - --------------------------------------  1/28 [urllib3]\n",
      "   - --------------------------------------  1/28 [urllib3]\n",
      "   - --------------------------------------  1/28 [urllib3]\n",
      "   -- -------------------------------------  2/28 [safetensors]\n",
      "   ---- -----------------------------------  3/28 [pyyaml]\n",
      "   ---- -----------------------------------  3/28 [pyyaml]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   ----- ----------------------------------  4/28 [pyarrow]\n",
      "   -------- -------------------------------  6/28 [multidict]\n",
      "   ---------- -----------------------------  7/28 [idna]\n",
      "   ----------- ----------------------------  8/28 [h11]\n",
      "  Attempting uninstall: fsspec\n",
      "   ----------- ----------------------------  8/28 [h11]\n",
      "    Found existing installation: fsspec 2025.12.0\n",
      "   ----------- ----------------------------  8/28 [h11]\n",
      "    Uninstalling fsspec-2025.12.0:\n",
      "   ----------- ----------------------------  8/28 [h11]\n",
      "      Successfully uninstalled fsspec-2025.12.0\n",
      "   ----------- ----------------------------  8/28 [h11]\n",
      "   ------------ ---------------------------  9/28 [fsspec]\n",
      "   ------------ ---------------------------  9/28 [fsspec]\n",
      "   ------------ ---------------------------  9/28 [fsspec]\n",
      "   ------------ ---------------------------  9/28 [fsspec]\n",
      "   ------------ ---------------------------  9/28 [fsspec]\n",
      "   ------------ ---------------------------  9/28 [fsspec]\n",
      "   --------------- ------------------------ 11/28 [dill]\n",
      "   --------------- ------------------------ 11/28 [dill]\n",
      "   --------------- ------------------------ 11/28 [dill]\n",
      "   --------------- ------------------------ 11/28 [dill]\n",
      "   --------------- ------------------------ 11/28 [dill]\n",
      "   --------------- ------------------------ 11/28 [dill]\n",
      "   ----------------- ---------------------- 12/28 [charset_normalizer]\n",
      "   ----------------- ---------------------- 12/28 [charset_normalizer]\n",
      "   -------------------- ------------------- 14/28 [attrs]\n",
      "   -------------------- ------------------- 14/28 [attrs]\n",
      "   --------------------- ------------------ 15/28 [aiohappyeyeballs]\n",
      "   ---------------------- ----------------- 16/28 [yarl]\n",
      "   ------------------------ --------------- 17/28 [requests]\n",
      "   ------------------------- -------------- 18/28 [multiprocess]\n",
      "   ------------------------- -------------- 18/28 [multiprocess]\n",
      "   ------------------------- -------------- 18/28 [multiprocess]\n",
      "   ------------------------- -------------- 18/28 [multiprocess]\n",
      "   ------------------------- -------------- 18/28 [multiprocess]\n",
      "   --------------------------- ------------ 19/28 [httpcore]\n",
      "   --------------------------- ------------ 19/28 [httpcore]\n",
      "   --------------------------- ------------ 19/28 [httpcore]\n",
      "   ---------------------------- ----------- 20/28 [anyio]\n",
      "   ---------------------------- ----------- 20/28 [anyio]\n",
      "   ---------------------------- ----------- 20/28 [anyio]\n",
      "   ---------------------------- ----------- 20/28 [anyio]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   ------------------------------- -------- 22/28 [huggingface-hub]\n",
      "   -------------------------------- ------- 23/28 [httpx]\n",
      "   -------------------------------- ------- 23/28 [httpx]\n",
      "   -------------------------------- ------- 23/28 [httpx]\n",
      "   ---------------------------------- ----- 24/28 [aiohttp]\n",
      "   ---------------------------------- ----- 24/28 [aiohttp]\n",
      "   ---------------------------------- ----- 24/28 [aiohttp]\n",
      "   ---------------------------------- ----- 24/28 [aiohttp]\n",
      "   ---------------------------------- ----- 24/28 [aiohttp]\n",
      "   ---------------------------------- ----- 24/28 [aiohttp]\n",
      "   ----------------------------------- ---- 25/28 [tokenizers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   ------------------------------------- -- 26/28 [transformers]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   -------------------------------------- - 27/28 [datasets]\n",
      "   ---------------------------------------- 28/28 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 attrs-25.4.0 certifi-2025.11.12 charset_normalizer-3.4.4 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 idna-3.11 multidict-6.7.0 multiprocess-0.70.18 propcache-0.4.1 pyarrow-22.0.0 pyyaml-6.0.3 requests-2.32.5 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3 urllib3-2.6.2 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d98353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from accelerate) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from accelerate) (2.9.1+cpu)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell g15\\rnn_and_transformers\\venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7485eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du device : cpu\n",
      "\n",
      " Dataset cr : custom_dataset.txt\n",
      "Chargement du modle GPT-2...\n",
      " Modle charg\n",
      "\n",
      "Dmarrage du fine-tuning (3 epochs)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/5 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1/3: 100%|| 5/5 [00:07<00:00,  1.41s/it, loss=0.866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss moyen : 3.1746\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|| 5/5 [00:05<00:00,  1.17s/it, loss=0.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss moyen : 0.9148\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|| 5/5 [00:05<00:00,  1.10s/it, loss=0.769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss moyen : 0.8072\n",
      "\n",
      "Sauvegarde du modle fine-tun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modle sauvegard dans ./gpt2_finetuned\n",
      "\n",
      "============================================================\n",
      "GNRATION DE TEXTE AVEC LE MODLE FINE-TUN\n",
      "============================================================\n",
      "\n",
      "Prompt : 'L'intelligence artificielle'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte gnr : L'intelligence artificielle d'apprentissage.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt : 'Les transformers sont'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte gnr : Les transformers sont rvolutiones de l'apprentissage d'conomie des dveloppementes.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt : 'La programmation en Python'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte gnr : La programmation en Python.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt : 'L'apprentissage profond'\n",
      "Texte gnr : L'apprentissage profondes de l'ducation des dveloppement des rvolutiones.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============ DATASET PERSONNALIS ============\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, text_file, block_size=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            self.examples = f.readlines()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.examples[idx].strip()\n",
    "        encodings = self.tokenizer(text, truncation=True, max_length=self.block_size, \n",
    "                                   padding='max_length', return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "# ============ CRER LE DATASET ============\n",
    "def create_custom_dataset():\n",
    "    texts = [\n",
    "        \"L'intelligence artificielle est une technologie rvolutionnaire qui change notre monde.\",\n",
    "        \"Les transformers sont des modles puissants pour le traitement du langage naturel.\",\n",
    "        \"GPT-2 est un modle de gnration de texte trs efficace et polyvalent.\",\n",
    "        \"L'apprentissage profond nous permet de crer des systmes intelligents et autonomes.\",\n",
    "        \"Les rseaux de neurones artificiels sont inspirs par le cerveau humain.\",\n",
    "        \"La science des donnes ouvre de nouvelles possibilits dans tous les domaines.\",\n",
    "        \"Les algorithmes d'apprentissage automatique peuvent prdire des tendances futures.\",\n",
    "        \"La programmation en Python est populaire pour l'intelligence artificielle.\",\n",
    "        \"Les modles pr-entrans comme GPT-2 facilitent le dveloppement d'applications.\",\n",
    "        \"L'entranement sur GPU acclre considrablement le processus d'apprentissage.\",\n",
    "    ]\n",
    "    \n",
    "    with open(\"custom_dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for text in texts:\n",
    "            f.write(text + \"\\n\")\n",
    "    \n",
    "    print(\" Dataset cr : custom_dataset.txt\")\n",
    "\n",
    "# ============ ENTRANEMENT MANUEL ============\n",
    "def finetune_gpt2():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Utilisation du device : {device}\\n\")\n",
    "    \n",
    "    # Crer dataset\n",
    "    create_custom_dataset()\n",
    "    \n",
    "    # Charger tokenizer et modle\n",
    "    print(\"Chargement du modle GPT-2...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.to(device)\n",
    "    print(\" Modle charg\\n\")\n",
    "    \n",
    "    # Prparer le dataset\n",
    "    dataset = TextDataset(tokenizer, \"custom_dataset.txt\", block_size=128)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    # Configuration d'entranement\n",
    "    epochs = 3\n",
    "    learning_rate = 5e-5\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(f\"Dmarrage du fine-tuning ({epochs} epochs)...\\n\")\n",
    "    \n",
    "    # Boucle d'entranement\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} - Loss moyen : {avg_loss:.4f}\\n\")\n",
    "    \n",
    "    # Sauvegarder le modle\n",
    "    print(\"Sauvegarde du modle fine-tun...\")\n",
    "    model.save_pretrained(\"./gpt2_finetuned\")\n",
    "    tokenizer.save_pretrained(\"./gpt2_finetuned\")\n",
    "    print(\" Modle sauvegard dans ./gpt2_finetuned\\n\")\n",
    "    \n",
    "    return model, tokenizer, device\n",
    "\n",
    "# ============ GNRATION DE TEXTE ============\n",
    "def generate_text(model, tokenizer, prompt, device, max_length=80):\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Prompt : '{prompt}'\")\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Texte gnr : {generated_text}\\n\")\n",
    "    return generated_text\n",
    "\n",
    "# ============ FONCTION PRINCIPALE ============\n",
    "if __name__ == \"__main__\":\n",
    "    # Fine-tuner le modle\n",
    "    model, tokenizer, device = finetune_gpt2()\n",
    "    \n",
    "    # Gnrer du texte\n",
    "    print(\"=\"*60)\n",
    "    print(\"GNRATION DE TEXTE AVEC LE MODLE FINE-TUN\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    prompts = [\n",
    "        \"L'intelligence artificielle\",\n",
    "        \"Les transformers sont\",\n",
    "        \"La programmation en Python\",\n",
    "        \"L'apprentissage profond\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        generate_text(model, tokenizer, prompt, device, max_length=80)\n",
    "        print(\"-\"*60 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
